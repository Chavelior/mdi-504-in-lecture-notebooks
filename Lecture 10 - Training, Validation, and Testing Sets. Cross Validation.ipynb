{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Lecture 10: Training, Validation and Testing Sets, Cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Linear least squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_POINTS = 10\n",
    "THETA_0 = 1\n",
    "THETA_1 = 5\n",
    "\n",
    "# generate 10 random values of x\n",
    "x = np.random.random(NUM_POINTS)\n",
    "# apply linear function, then plot\n",
    "y = THETA_0 + THETA_1*x\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add iid noise N(0, 1^2) to y values to form\n",
    "# noisy observations \n",
    "SIGMA_W = 1\n",
    "w = np.random.normal(0, SIGMA_W, NUM_POINTS)\n",
    "y_obs = y + w\n",
    "\n",
    "# plot noisy data\n",
    "plt.figure()\n",
    "plt.scatter(x, y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basis functions:\n",
    "# phi_0(x) = 1\n",
    "# phi_1(x) = x\n",
    "\n",
    "phi_0 = np.ones(x.shape)\n",
    "phi_1 = x\n",
    "\n",
    "# Design matrix, phi as columns\n",
    "Phi = np.array([phi_0, phi_1])\n",
    "\n",
    "# This shows we have the phi as rows\n",
    "# we don't want that.\n",
    "print(Phi.shape)\n",
    "\n",
    "# Take transpose to get phi as columns\n",
    "Phi = Phi.transpose()\n",
    "print(Phi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moore-Penrose pseudoinverse\n",
    "A = np.matmul(Phi.transpose(), Phi)\n",
    "A_inv = np.linalg.inv(A)\n",
    "Phi_inv = np.matmul(A_inv,Phi.transpose())\n",
    "theta_ls = np.matmul(Phi_inv, y_obs)\n",
    "print(theta_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In reality, calling np.linalg.inv isn't great. \n",
    "# It's numerically unstable, and inefficient. We really \n",
    "# want to solve the normal equations, and there's \n",
    "# better ways to do this, all built into the np.linalg.solve\n",
    "# function:\n",
    "\n",
    "A = np.matmul(Phi.transpose(), Phi)\n",
    "b = np.matmul(Phi.transpose(), y_obs)\n",
    "theta_ls = np.linalg.solve(A, b)\n",
    "print(theta_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice, you should probably use the least squares\n",
    "# solver that comes with numpy. You can call it with \n",
    "# np.linalg.lstsq:\n",
    "\n",
    "res = np.linalg.lstsq(Phi, y_obs, rcond=None)\n",
    "theta_ls = res[0]\n",
    "print(theta_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the true function, the least-squares \n",
    "# fit and the observed data points all in the same plot\n",
    "\n",
    "# the set of x we will plot the true and fitted model against\n",
    "x_plot = np.linspace(0, 1, 5)\n",
    "# the y-values of the true model\n",
    "y_true = THETA_0 + THETA_1*x_plot\n",
    "# the y-values of the least squares fitted model\n",
    "y_model = theta_ls[0] + theta_ls[1]*x_plot\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_plot, y_true)\n",
    "plt.plot(x_plot, y_model)\n",
    "# plot the observations as well\n",
    "plt.errorbar(x, y_obs, yerr=SIGMA_W, fmt='o')\n",
    "\n",
    "plt.legend(['True', 'Least Squares Fit', 'Observations'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll load data from a text file\n",
    "data = np.loadtxt('data.txt')\n",
    "# We see that there are 2 columns (x, y) and 30 data points\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the data:\n",
    "x_data = data[:,0]\n",
    "y_data = data[:,1]\n",
    "plt.figure()\n",
    "plt.scatter(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll again use the monomial basis, i.e.\n",
    "# phi_n = x^n for n = 0, 1, ..., NUM_BASIS-1\n",
    "\n",
    "# Hint: If we're using the monomial basis, the \n",
    "# corresponding design matrix Phi has a special\n",
    "# name called the Vandermonde matrix, and \n",
    "# numpy has a special function to construct this matrix\n",
    "# called np.vander\n",
    "\n",
    "# fix the number of basis elements to use\n",
    "NUM_BASIS = 4\n",
    "# Form the design matrix, aka Vandermonde matrix\n",
    "Phi = np.vander(x_data, NUM_BASIS, increasing = True)\n",
    "# Solve for the least squares solution\n",
    "theta_ls = np.linalg.lstsq(Phi, y_data, rcond=None)[0]\n",
    "print(Phi.shape)\n",
    "print(theta_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fitted linear model with data\n",
    "\n",
    "# Pick a bunch of x values to evaluate the model\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "\n",
    "# Evaluate each basis element at every point \n",
    "Phi_plot = np.vander(x_plot, NUM_BASIS, increasing = True)\n",
    "\n",
    "# Evaluate the model y = Phi*theta\n",
    "y_plot = np.matmul(Phi_plot, theta_ls)\n",
    "\n",
    "# Plot the data points and the model fit\n",
    "plt.figure()\n",
    "plt.scatter(x_data, y_data)\n",
    "plt.plot(x_plot, y_plot, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residual with respect to data\n",
    "Phi_model = np.vander(x_data, NUM_BASIS, increasing = True)\n",
    "y_model = np.matmul(Phi_model, theta_ls)\n",
    "r = y_model - y_data\n",
    "print(\"Norm of residual = \" + str(np.linalg.norm(r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Vaidation and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = data[0:20, :]\n",
    "x_training = training_set[:, 0]\n",
    "y_training = training_set[:, 1]\n",
    "\n",
    "validation_set = data[20:24, :]\n",
    "x_validation = validation_set[:, 0]\n",
    "y_validation = validation_set[:, 1]\n",
    "\n",
    "testing_set = data[24:30, :]\n",
    "x_testing = testing_set[:, 0]\n",
    "y_testing = testing_set[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll consider 9 models. Each model will use \n",
    "# the monomial basis elements {phi_n = x^n}\n",
    "# The only difference between models is how many\n",
    "# of these elements to include. \n",
    "\n",
    "models_N = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "model_performance = np.zeros(len(models_N))\n",
    "\n",
    "# loop over each model\n",
    "for (i,N) in enumerate(models_N):\n",
    "    # train model N (aka N basis elements) with training set\n",
    "    Phi = np.vander(x_training, N, increasing = True)\n",
    "    theta_ls = np.linalg.lstsq(Phi, y_training, rcond=None)[0]\n",
    "    \n",
    "    # calculate performance with validation set\n",
    "    Phi_model = np.vander(x_validation, N, increasing = True)\n",
    "    y_model = np.matmul(Phi_model, theta_ls)\n",
    "    \n",
    "    # compare model prediction with validation data\n",
    "    r = y_model - y_validation\n",
    "    \n",
    "    # save performance, here we'll use MSE\n",
    "    model_performance[i] = np.mean(r**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each model's performance\n",
    "plt.figure()\n",
    "plt.plot(models_N, model_performance)\n",
    "\n",
    "# Pick best performing model\n",
    "i_best = np.argmin(model_performance)\n",
    "N_best = models_N[i_best]\n",
    "print(\"Best model: N = \" + str(N_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain best model with training set\n",
    "Phi = np.vander(x_training, N_best, increasing = True)\n",
    "theta_ls = np.linalg.lstsq(Phi, y_training, rcond=None)[0]\n",
    "      \n",
    "# Now compare the final performance of the best model against testing set\n",
    "Phi_model = np.vander(x_testing, N_best, increasing = True)\n",
    "y_model = np.matmul(Phi_model, theta_ls)\n",
    "r = y_model - y_testing\n",
    "\n",
    "print(\"MSE for testing set: \" + str(np.mean(r**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k fold cross-validation\n",
    "training_set = data[0:24, :]\n",
    "\n",
    "testing_set = data[24:30, :]\n",
    "x_testing = testing_set[:, 0]\n",
    "y_testing = testing_set[:, 1]\n",
    "\n",
    "# pick number of subsets of training sets\n",
    "# here we'll pick 3 subsets, each with 8 data points\n",
    "k = 3\n",
    "\n",
    "# We'll define the 3 sets as\n",
    "# subset 0 = data points 0, 3, 6,  9, 12, 15, 18, 21 (mod 4 = 0)\n",
    "# subset 1 = data points 1, 4, 7, 10, 13, 16, 19, 22 (mod 4 = 1)\n",
    "# subset 2 = data points 2, 5, 8, 11, 14, 17, 20, 23 (mod 4 = 2)\n",
    "I = np.arange(24)\n",
    "\n",
    "# loop over models\n",
    "for (i,N) in enumerate(models_N):\n",
    "\n",
    "    # perform k validations and save performance for each validation    \n",
    "    model_performance_j = np.zeros(k)\n",
    "    for j in range(k):\n",
    "        # define j-th validation and training sets\n",
    "        # using boolean indexing\n",
    "        validation_set_j = training_set[(I % k) == j, :]\n",
    "        training_set_j = training_set[(I % k) != j, :]\n",
    "        \n",
    "        # pick out input and output vectors from training set\n",
    "        x_training_j = training_set_j[:, 0]\n",
    "        y_training_j = training_set_j[:, 1]\n",
    "        \n",
    "        # pick out input and output vectors from validation set\n",
    "        x_validation_j = validation_set_j[:, 0]\n",
    "        y_validation_j = validation_set_j[:, 1]\n",
    "        \n",
    "        # train with training set\n",
    "        Phi = np.vander(x_training_j, N, increasing = True)\n",
    "        theta_ls = np.linalg.lstsq(Phi, y_training_j, rcond=None)[0]\n",
    "        \n",
    "        # calculate residual with validation set\n",
    "        Phi_model = np.vander(x_validation_j, N, increasing = True)\n",
    "        y_model = np.matmul(Phi_model, theta_ls)\n",
    "        r = y_model - y_validation_j\n",
    "        \n",
    "        # save performance for the j-th validation set\n",
    "        model_performance_j[j] = np.mean(r**2)\n",
    "        \n",
    "    # Overall model performance for the i-th model is \n",
    "    # just the average over all validation sets\n",
    "    model_performance[i] = np.mean(model_performance_j)\n",
    "    \n",
    "# At the end of this loop, model_performance will hold \n",
    "# the validation performance for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each model's performance after cross validation\n",
    "plt.figure()\n",
    "plt.plot(models_N, model_performance)\n",
    "\n",
    "# Pick best performing model\n",
    "i_best = np.argmin(model_performance)\n",
    "N_best = models_N[i_best]\n",
    "print(\"Best model: N = \" + str(N_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain the best model with training set\n",
    "Phi = np.vander(x_training, N_best, increasing = True)\n",
    "theta_ls = np.linalg.lstsq(Phi, y_training, rcond=None)[0]\n",
    "      \n",
    "# Now compare the final performance of the best model against testing set\n",
    "Phi_model = np.vander(x_testing, N_best, increasing = True)\n",
    "y_model = np.matmul(Phi_model, theta_ls)\n",
    "r = y_model - y_testing\n",
    "\n",
    "print(\"MSE for testing set: \" + str(np.mean(r**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
