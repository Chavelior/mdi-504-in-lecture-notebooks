{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    "\n",
    "We would like to restrict the space of least-square solutions $\\vec \\theta$, which minimize the least-squares loss function\n",
    "\n",
    "$$ \\large L_\\text{LS}(\\vec \\theta) = \\| \\vec y - \\Phi \\vec \\theta \\|^2_2,$$\n",
    "\n",
    "where $\\Phi$ is the $m \\times n$ design matrix, $\\vec y$ is the observation vector and $\\vec \\theta$ is the vector of coefficients we are trying to solve for. The least-squares solution is the $\\vec \\theta$ that minimizes this:\n",
    "\n",
    "$$ \\large \\vec\\theta_\\text{LS} = \\arg\\min L_\\text{LS}(\\vec \\theta).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two regularization techniques:\n",
    "* L1 regularization (aka **Lasso**):\n",
    "$$\\large L_\\text{Lasso}(\\vec \\theta) = \\| \\vec y - \\Phi \\vec \\theta \\|_2^2 + \\lambda \\| \\vec \\theta\\|_1.\\\\ $$\n",
    "\n",
    "* L2 regularization (aka **Ridge (or Tikhonov) Regularization**):\n",
    "$$\\large L_\\text{Ridge}(\\vec \\theta) = \\| \\vec y - \\Phi \\vec \\theta \\|_2 ^2 + \\lambda \\| \\vec \\theta\\|_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- make nice figures -----\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 200\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "x = np.linspace(0, 1, 10)\n",
    "y = 1 + 10*x - 50*x**2 +1000*x**3 - 1000*x**4\n",
    "W = np.random.normal(0, 10, len(y))\n",
    "y_obs = y + W\n",
    "\n",
    "\n",
    "plt.scatter(x, y_obs, color='r')\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to fit a degree 9 polynomial\n",
    "\n",
    "Phi = np.vander(x, 11, increasing=True)\n",
    "\n",
    "# Print out rank for our information\n",
    "rank = np.linalg.matrix_rank(Phi)\n",
    "num_cols = Phi.shape[1]\n",
    "print(\"Phi has rank = \" + str(rank))\n",
    "print(\"n = \" + str(num_cols))\n",
    "\n",
    "# Get least squares solution\n",
    "theta_ls = np.linalg.lstsq(Phi, y_obs, rcond=None)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a more refined plot of the model\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "Phi_plot = np.vander(x_plot, 11, increasing=True)\n",
    "# @ is matrix multiplication\n",
    "y_model = Phi_plot @ theta_ls\n",
    "y_plot = 1 + 10*x_plot - 50*x_plot**2 +1000*x_plot**3 - 1000*x_plot**4\n",
    "\n",
    "\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.plot(x_plot, y_model)\n",
    "plt.scatter(x, y_obs, color='red')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Truth', 'Least-squares fit', 'Observations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the above that this is an overfit. A typical outcome of overfitting is that the coefficients are take on unrealistic values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What theta values do we get?\n",
    "plt.plot(theta_ls, linewidth=0, marker='o', markersize=5)\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('theta_i')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to control or penalize large values of theta to avoid this type of behavior. So instead of minimizing least squares loss, we'll introduce a penalty term for large coefficient values:\n",
    "\n",
    "$$\\large L_\\text{Ridge}(\\vec \\theta) = \\| \\vec y - \\Phi \\vec \\theta \\|_2 ^2 + \\lambda \\| \\vec \\theta\\|_2^2 $$\n",
    "\n",
    "We'll use the library: `sklearn.linear_model.Ridge` for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# We'll do this in three steps. First we set up the model, then we fit it. Then we get the coefficients\n",
    "\n",
    "# 1. set up model. Here alpha is what we've been calling lambda above\n",
    "ridge_model = Ridge(alpha = 1.0)\n",
    "\n",
    "# 2. Fit the model\n",
    "ridge_model.fit(Phi, y_obs)\n",
    "\n",
    "# 3. Get coefficients\n",
    "theta_ridge = ridge_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(theta_ridge, linewidth=0, marker='o', markersize=5)\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('theta_i')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model = Phi_plot @ theta_ridge\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.plot(x_plot, y_model)\n",
    "plt.scatter(x, y_obs, color='red')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Truth', 'Ridge regression fit', 'Observations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try lowering penality.\n",
    "ridge_model = Ridge(alpha = 0)\n",
    "ridge_model.fit(Phi, y_obs)\n",
    "theta_ridge = ridge_model.coef_\n",
    "\n",
    "# plot theta\n",
    "plt.plot(theta_ridge, linewidth=0, marker='o', markersize=5)\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('theta_i')\n",
    "plt.grid()\n",
    "\n",
    "# plot model\n",
    "y_model = Phi_plot @ theta_ridge\n",
    "plt.figure()\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.plot(x_plot, y_model)\n",
    "plt.scatter(x, y_obs, color='red')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Truth', 'Ridge regression fit', 'Observations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-normalization\n",
    "mu_y = np.mean(y_obs)\n",
    "sig_y = np.std(y_obs)\n",
    "y_norm = (y_obs - mu_y)/sig_y\n",
    "\n",
    "mu_Phi = np.mean(Phi)\n",
    "sig_Phi = np.std(Phi)\n",
    "Phi_norm = (Phi - mu_Phi)/sig_Phi\n",
    "\n",
    "# Fit intercept set to false here since we've normalized our data\n",
    "ridge_model = Ridge(alpha = 0.01, fit_intercept=False)\n",
    "ridge_model.fit(Phi_norm, y_norm)\n",
    "theta_ridge = ridge_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot theta\n",
    "plt.plot(theta_ridge, linewidth=0, marker='o', markersize=5)\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('theta_i')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply same transformation to Phi_plot\n",
    "Phi_plot_norm = (Phi_plot - mu_Phi) / sig_Phi\n",
    "\n",
    "# Apply same transformation to y_plot\n",
    "y_plot_norm = (y_plot - mu_y)/sig_y\n",
    "\n",
    "\n",
    "# plot model\n",
    "y_model = Phi_plot_norm @ theta_ridge\n",
    "plt.figure()\n",
    "plt.plot(x_plot, y_plot_norm)\n",
    "plt.plot(x_plot, y_model)\n",
    "plt.scatter(x, y_norm, color='red')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['Truth', 'Ridge regression fit', 'Observations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso\n",
    "\n",
    "\n",
    "Lasso is very similar to Ridge, except that we penalize the 1-norm of the coefficients, which we've seen induces sparsity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.5*np.sin(x) + 0.2*np.sin(33.0*x) + 0.2*np.sin(75.0*x)\n",
    "    \n",
    "x = np.linspace(0, 4*np.pi, 100)\n",
    "y = f(x)\n",
    "W = np.random.normal(0, 0.2, len(y))\n",
    "y_obs = y + W\n",
    "\n",
    "\n",
    "x_plot = np.linspace(0, 4*np.pi, 200)\n",
    "y_plot = f(x_plot)\n",
    "\n",
    "plt.plot(x_plot, y_plot)\n",
    "plt.scatter(x, y_obs, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_fourier_design_matrix(X, n):\n",
    "    # phi_ij = sin(j*x_i) for j = 0, ..., n-1\n",
    "    \n",
    "    num_rows = X.shape[0]\n",
    "    num_cols = n\n",
    "    \n",
    "    Phi = np.zeros([num_rows, num_cols])\n",
    "    \n",
    "    # iterate over data points\n",
    "    for i, x in enumerate(X):\n",
    "        \n",
    "        # iterate over 1, 2, ..., n\n",
    "        for j in range(n):\n",
    "            \n",
    "            Phi[i,j] = np.sin(j*x)\n",
    "            \n",
    "            \n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "NUM_BASIS = 100\n",
    "Phi = form_fourier_design_matrix(x, NUM_BASIS)\n",
    "\n",
    "# normalize\n",
    "mu_y = np.mean(y_obs)\n",
    "sig_y = np.std(y_obs)\n",
    "y_norm = (y_obs - mu_y)/sig_y\n",
    "\n",
    "mu_Phi = np.mean(Phi)\n",
    "sig_Phi = np.std(Phi)\n",
    "Phi_norm = (Phi - mu_Phi)/sig_Phi\n",
    "\n",
    "lasso_model = Lasso(alpha = 0.1, max_iter = 10000, fit_intercept=False)\n",
    "#lasso_model = LassoCV(fit_intercept=False, positive=True)\n",
    "lasso_model.fit(Phi_norm, y_norm)\n",
    "theta_lasso =  lasso_model.coef_\n",
    "\n",
    "# ridge regression solution for comparison\n",
    "ridge_model = Ridge(alpha = 0.1, fit_intercept=False)\n",
    "ridge_model.fit(Phi_norm, y_norm)\n",
    "theta_ridge = ridge_model.coef_\n",
    "\n",
    "# aply models\n",
    "Phi_plot = form_fourier_design_matrix(x_plot, NUM_BASIS)\n",
    "Phi_plot = (Phi_plot - mu_Phi)/sig_Phi\n",
    "y_lasso = Phi_plot @ theta_lasso\n",
    "y_ridge = Phi_plot @ theta_ridge\n",
    "\n",
    "plt.plot(x_plot, y_lasso)\n",
    "plt.plot(x_plot, y_ridge, ls=':')\n",
    "plt.scatter(x, y_norm, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(theta_lasso)\n",
    "plt.plot(theta_ridge, ls=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso_model = LassoCV(fit_intercept=False)\n",
    "lasso_model.fit(Phi_norm, y_norm)\n",
    "theta_lasso =  lasso_model.coef_\n",
    "\n",
    "y_lasso = Phi_plot @ theta_lasso\n",
    "\n",
    "plt.plot(x_plot, y_lasso)\n",
    "plt.plot(x_plot, y_ridge, ls=':')\n",
    "plt.scatter(x, y_norm, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(theta_lasso)\n",
    "plt.plot(theta_ridge, ls=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = LassoCV(fit_intercept=False, positive = True)\n",
    "lasso_model.fit(Phi_norm, y_norm)\n",
    "theta_lasso =  lasso_model.coef_\n",
    "\n",
    "Phi_plot = form_fourier_design_matrix(x_plot, NUM_BASIS)\n",
    "Phi_plot = (Phi_plot - mu_Phi)/sig_Phi\n",
    "y_lasso = Phi_plot @ theta_lasso\n",
    "y_ridge = Phi_plot @ theta_ridge\n",
    "\n",
    "plt.plot(theta_lasso)\n",
    "plt.plot(theta_ridge, ls=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "en_model = ElasticNetCV(fit_intercept=False, positive=True)\n",
    "en_model.fit(Phi_norm, y_norm)\n",
    "theta_en =  en_model.coef_\n",
    "\n",
    "y_en = Phi_plot @ theta_en\n",
    "\n",
    "plt.plot(theta_en)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
